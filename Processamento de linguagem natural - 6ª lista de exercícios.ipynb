{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Universidade Federal de Alagoas\n",
    "\n",
    "IC - Instituto de Computação\n",
    "\n",
    " \n",
    "\n",
    "# Processamento de linguagem natural - 2020.1\n",
    "**Professor**: Thales Vieira\n",
    "\n",
    "**Alunos**: Hugo Tallys Martins Oliveira e Valério Nogueira Rodrigues Júnior\n",
    "\n",
    "\n",
    "## 6ª lista de exercícios\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pré-processamento dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\valerio nogueira\\python 3.8.5\\lib\\site-packages\\sklearn\\utils\\deprecation.py:143: FutureWarning: The sklearn.metrics.classification module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy\n",
    "import pandas\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import umap\n",
    "\n",
    "from bokeh.io import output_notebook, show\n",
    "from bokeh.palettes import Category20\n",
    "from bokeh.plotting import figure\n",
    "from gensim.models import KeyedVectors\n",
    "from IPython.display import HTML, display\n",
    "from keras.layers import Conv1D, Dense, Embedding, LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD, NMF, PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import normalize\n",
    "from yellowbrick.cluster import KElbowVisualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1001\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1001\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.1.1.min.js\": \"kLr4fYcqcSpbuI95brIH3vnnYCquzzSxHPU6XGQCIkQRGJwhg0StNbj1eegrHs12\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.1.1.min.js\": \"xIGPmVtaOm+z0BqfSOMn4lOR6ciex448GIKG4eE61LsAvmGj48XcMQZtKcE/UXZe\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.1.1.min.js\": \"Dc9u1wF/0zApGIWoBbH77iWEHtdmkuYWG839Uzmv8y8yBLXebjO9ZnERsde5Ln/P\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.1.1.min.js\": \"cT9JaBz7GiRXdENrJLZNSC6eMNF3nh3fa5fTF51Svp+ukxPdwcU5kGXGPBgDCa2j\"};\n",
       "\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      if (url in hashes) {\n",
       "        element.crossOrigin = \"anonymous\";\n",
       "        element.integrity = \"sha384-\" + hashes[url];\n",
       "      }\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  \n",
       "  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.1.1.min.js\"];\n",
       "  var css_urls = [];\n",
       "  \n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "    \n",
       "    \n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      \n",
       "    for (var i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "    if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.1.1.min.js\": \"kLr4fYcqcSpbuI95brIH3vnnYCquzzSxHPU6XGQCIkQRGJwhg0StNbj1eegrHs12\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.1.1.min.js\": \"xIGPmVtaOm+z0BqfSOMn4lOR6ciex448GIKG4eE61LsAvmGj48XcMQZtKcE/UXZe\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.1.1.min.js\": \"Dc9u1wF/0zApGIWoBbH77iWEHtdmkuYWG839Uzmv8y8yBLXebjO9ZnERsde5Ln/P\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.1.1.min.js\": \"cT9JaBz7GiRXdENrJLZNSC6eMNF3nh3fa5fTF51Svp+ukxPdwcU5kGXGPBgDCa2j\"};\n\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      if (url in hashes) {\n        element.crossOrigin = \"anonymous\";\n        element.integrity = \"sha384-\" + hashes[url];\n      }\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.1.1.min.js\"];\n  var css_urls = [];\n  \n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (var i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_notebook() # Necessário para visualizar os gráficos com bokeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Valerio\n",
      "[nltk_data]     Nogueira\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package rslp to C:\\Users\\Valerio\n",
      "[nltk_data]     Nogueira\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Valerio\n",
      "[nltk_data]     Nogueira\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords'); nltk.download('rslp'); nltk.download('punkt');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bbc_dataset_url = 'data/bbc.csv'\n",
    "cnn_dataset_url = 'data/cnn.csv'\n",
    "\n",
    "bbc = pandas.read_csv(bbc_dataset_url, sep='|')\n",
    "bbc['label'] = 'BBC'\n",
    "cnn = pandas.read_csv(cnn_dataset_url, sep='|')\n",
    "cnn['label'] = 'CNN'\n",
    "\n",
    "dataset = pandas.concat([bbc, cnn], ignore_index=True)\n",
    "dataset = dataset.dropna(axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset.text = dataset.text.apply(lambda text: text.replace('\\n', ' ')) # Remoção das quebras de linha\n",
    "dataset.title = dataset.title.apply(lambda text: text.replace('\\n', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def remove_boilerplate(text):\n",
    "    boilerplate = ['Compartilhe este post com Email Facebook Messenger Messenger Twitter WhatsApp LinkedIn Copiar este link Estes são links externos e abrirão numa nova janela', 'Já assistiu aos nossos novos vídeos no YouTube? Inscreva-se no nosso canal!', 'Final de YouTube post  de BBC News Brasil Final de YouTube post 2 de BBC News Brasil Final de YouTube post 3 de BBC News Brasil']\n",
    "    \n",
    "    for b in boilerplate:\n",
    "        text = text.replace(b, '')\n",
    "    return text\n",
    "\n",
    "dataset.text = dataset.text.apply(remove_boilerplate) # Remoção de fragmentos irrelevantes do texto que se repetem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = re.sub(r'\\w*\\d\\w*', '', text) # Remove todas as palavras que contém números\n",
    "    text = re.sub(r'[^a-zA-ZáéíóúÁÉÍÓÚâêîôÂÊÎÔãõÃÕçÇ ]', '', text.lower()) # Remove pontuação e converte para minúscula\n",
    "    return re.sub(r'\\s+', ' ', text) # Remove espaços repetidos\n",
    "\n",
    "dataset['processed_text'] = dataset.text.apply(preprocess)\n",
    "dataset['processed_title'] = dataset.title.apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "\n",
    "def tokenize_remove_stopwords(text):\n",
    "    tokenized_text = nltk.word_tokenize(text, language='portuguese')\n",
    "    return \" \".join([token for token in tokenized_text if token not in stopwords])\n",
    "\n",
    "dataset.processed_text = dataset.processed_text.apply(tokenize_remove_stopwords) # Tokeniza o texto e remove stopwords\n",
    "dataset.processed_title = dataset.processed_title.apply(tokenize_remove_stopwords) # Tokeniza o titulo e remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>processed_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.bbc.co.uk/portuguese/brasil-53020785</td>\n",
       "      <td>Coronavírus: pandemia pode jogar até 14 milhõe...</td>\n",
       "      <td>A turbulência econômica causada pela pandemi...</td>\n",
       "      <td>BBC</td>\n",
       "      <td>turbulência econômica causada pandemia novo co...</td>\n",
       "      <td>coronavírus pandemia pode jogar milhões brasil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.bbc.co.uk/portuguese/brasil-53027318</td>\n",
       "      <td>Coronavírus: como funcionam as duas vacinas co...</td>\n",
       "      <td>Cerca de 11 mil voluntários brasileiros vão ...</td>\n",
       "      <td>BBC</td>\n",
       "      <td>cerca mil voluntários brasileiros vão receber ...</td>\n",
       "      <td>coronavírus funcionam duas vacinas contra covi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.bbc.co.uk/portuguese/brasil-51713943</td>\n",
       "      <td>Coronavírus: Brasil passa o Reino Unido e se t...</td>\n",
       "      <td>*atualizada às 18h20 de 12 de junho de 2020 ...</td>\n",
       "      <td>BBC</td>\n",
       "      <td>atualizada s junho brasil totalizou nesta sext...</td>\n",
       "      <td>coronavírus brasil passa reino unido torna seg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.bbc.co.uk/portuguese/internacional...</td>\n",
       "      <td>Coronavírus na Índia: com lockdown 'insustentá...</td>\n",
       "      <td>Quando, em 24 de março, o governo indiano in...</td>\n",
       "      <td>BBC</td>\n",
       "      <td>março governo indiano iniciou estrito isolamen...</td>\n",
       "      <td>coronavírus índia lockdown insustentável índia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.bbc.co.uk/portuguese/internacional...</td>\n",
       "      <td>2ª onda do coronavírus? Irã vê aumento acelera...</td>\n",
       "      <td>O Irã registrou um rápido aumento no número ...</td>\n",
       "      <td>BBC</td>\n",
       "      <td>irã registrou rápido aumento número casos covi...</td>\n",
       "      <td>onda coronavírus irã vê aumento acelerado após...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1611</th>\n",
       "      <td>https://www.cnnbrasil.com.br/saude/2020/02/27/...</td>\n",
       "      <td>Farmácias têm falta de máscaras após confirmaç...</td>\n",
       "      <td>Com a confirmação do primeiro caso de contamin...</td>\n",
       "      <td>CNN</td>\n",
       "      <td>confirmação primeiro caso contaminação novo co...</td>\n",
       "      <td>farmácias têm falta máscaras após confirmação ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1612</th>\n",
       "      <td>https://www.cnnbrasil.com.br/business/2020/02/...</td>\n",
       "      <td>Ibovespa tem nova queda com mercado ainda preo...</td>\n",
       "      <td>Preocupações com a propagação do novo coronaví...</td>\n",
       "      <td>CNN</td>\n",
       "      <td>preocupações propagação novo coronavírus poten...</td>\n",
       "      <td>ibovespa nova queda mercado ainda preocupado c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1613</th>\n",
       "      <td>https://www.cnnbrasil.com.br/internacional/202...</td>\n",
       "      <td>Japonesa testa positivo pela segunda vez para ...</td>\n",
       "      <td>TÓQUIO - Uma guia de ônibus turístico no Japão...</td>\n",
       "      <td>CNN</td>\n",
       "      <td>tóquio guia ônibus turístico japão apresentou ...</td>\n",
       "      <td>japonesa testa positivo segunda vez coronavírus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1614</th>\n",
       "      <td>https://www.cnnbrasil.com.br/saude/2020/02/26/...</td>\n",
       "      <td>Primeiro brasileiro com coronavírus tem sintom...</td>\n",
       "      <td>O primeiro brasileiro com diagnóstico confir...</td>\n",
       "      <td>CNN</td>\n",
       "      <td>primeiro brasileiro diagnóstico confirmado cor...</td>\n",
       "      <td>primeiro brasileiro coronavírus sintomas brand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1615</th>\n",
       "      <td>https://www.cnnbrasil.com.br/saude/2020/02/26/...</td>\n",
       "      <td>Coronavírus: 'Não é momento para pânico' no Br...</td>\n",
       "      <td>O Brasil está preparado para lidar com o cor...</td>\n",
       "      <td>CNN</td>\n",
       "      <td>brasil preparado lidar coronavírus afirmou méd...</td>\n",
       "      <td>coronavírus momento pânico brasil diz médico s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1616 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    url  \\\n",
       "0      https://www.bbc.co.uk/portuguese/brasil-53020785   \n",
       "1      https://www.bbc.co.uk/portuguese/brasil-53027318   \n",
       "2      https://www.bbc.co.uk/portuguese/brasil-51713943   \n",
       "3     https://www.bbc.co.uk/portuguese/internacional...   \n",
       "4     https://www.bbc.co.uk/portuguese/internacional...   \n",
       "...                                                 ...   \n",
       "1611  https://www.cnnbrasil.com.br/saude/2020/02/27/...   \n",
       "1612  https://www.cnnbrasil.com.br/business/2020/02/...   \n",
       "1613  https://www.cnnbrasil.com.br/internacional/202...   \n",
       "1614  https://www.cnnbrasil.com.br/saude/2020/02/26/...   \n",
       "1615  https://www.cnnbrasil.com.br/saude/2020/02/26/...   \n",
       "\n",
       "                                                  title  \\\n",
       "0     Coronavírus: pandemia pode jogar até 14 milhõe...   \n",
       "1     Coronavírus: como funcionam as duas vacinas co...   \n",
       "2     Coronavírus: Brasil passa o Reino Unido e se t...   \n",
       "3     Coronavírus na Índia: com lockdown 'insustentá...   \n",
       "4     2ª onda do coronavírus? Irã vê aumento acelera...   \n",
       "...                                                 ...   \n",
       "1611  Farmácias têm falta de máscaras após confirmaç...   \n",
       "1612  Ibovespa tem nova queda com mercado ainda preo...   \n",
       "1613  Japonesa testa positivo pela segunda vez para ...   \n",
       "1614  Primeiro brasileiro com coronavírus tem sintom...   \n",
       "1615  Coronavírus: 'Não é momento para pânico' no Br...   \n",
       "\n",
       "                                                   text label  \\\n",
       "0       A turbulência econômica causada pela pandemi...   BBC   \n",
       "1       Cerca de 11 mil voluntários brasileiros vão ...   BBC   \n",
       "2       *atualizada às 18h20 de 12 de junho de 2020 ...   BBC   \n",
       "3       Quando, em 24 de março, o governo indiano in...   BBC   \n",
       "4       O Irã registrou um rápido aumento no número ...   BBC   \n",
       "...                                                 ...   ...   \n",
       "1611  Com a confirmação do primeiro caso de contamin...   CNN   \n",
       "1612  Preocupações com a propagação do novo coronaví...   CNN   \n",
       "1613  TÓQUIO - Uma guia de ônibus turístico no Japão...   CNN   \n",
       "1614    O primeiro brasileiro com diagnóstico confir...   CNN   \n",
       "1615    O Brasil está preparado para lidar com o cor...   CNN   \n",
       "\n",
       "                                         processed_text  \\\n",
       "0     turbulência econômica causada pandemia novo co...   \n",
       "1     cerca mil voluntários brasileiros vão receber ...   \n",
       "2     atualizada s junho brasil totalizou nesta sext...   \n",
       "3     março governo indiano iniciou estrito isolamen...   \n",
       "4     irã registrou rápido aumento número casos covi...   \n",
       "...                                                 ...   \n",
       "1611  confirmação primeiro caso contaminação novo co...   \n",
       "1612  preocupações propagação novo coronavírus poten...   \n",
       "1613  tóquio guia ônibus turístico japão apresentou ...   \n",
       "1614  primeiro brasileiro diagnóstico confirmado cor...   \n",
       "1615  brasil preparado lidar coronavírus afirmou méd...   \n",
       "\n",
       "                                        processed_title  \n",
       "0     coronavírus pandemia pode jogar milhões brasil...  \n",
       "1     coronavírus funcionam duas vacinas contra covi...  \n",
       "2     coronavírus brasil passa reino unido torna seg...  \n",
       "3     coronavírus índia lockdown insustentável índia...  \n",
       "4     onda coronavírus irã vê aumento acelerado após...  \n",
       "...                                                 ...  \n",
       "1611  farmácias têm falta máscaras após confirmação ...  \n",
       "1612  ibovespa nova queda mercado ainda preocupado c...  \n",
       "1613    japonesa testa positivo segunda vez coronavírus  \n",
       "1614  primeiro brasileiro coronavírus sintomas brand...  \n",
       "1615  coronavírus momento pânico brasil diz médico s...  \n",
       "\n",
       "[1616 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_vocab(texts):\n",
    "    vocab = ' '.join([text for text in texts])\n",
    "    return sorted(set(vocab.split(' ')))\n",
    "\n",
    "processed_texts_vocab = get_vocab(dataset.processed_text.values)\n",
    "processed_titles_vocab = get_vocab(dataset.processed_title.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER_NUM_WORDS = 2000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=TOKENIZER_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(dataset.processed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding, convoluções e LSTM\n",
    "\n",
    "Resolva novamente a segunda questão da 3ª lista usando pelo menos duas\n",
    "arquiteturas de redes neurais que utilizem camadas *Embedding*, convolucionais e\n",
    "*LSTM*. Compare com os resultados obtidos anteriormente nas lista 3 e 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gerando as sequências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(dataset.processed_text)\n",
    "sequences = pad_sequences(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIMENSION = TOKENIZER_NUM_WORDS + 1\n",
    "INPUT_LENGTH = 500\n",
    "CONV_FILTERS = 32\n",
    "CONV_KERNEL_SIZE = 3\n",
    "LSTM_UNITS = 100\n",
    "DENSE_UNITS = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 300)         1800      \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, None, 32)          28832     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                1616      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 85,465\n",
      "Trainable params: 85,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "lstm_model = Sequential([\n",
    "    Embedding(input_dim=INPUT_DIMENSION, output_dim=300),\n",
    "    Conv1D(filters=CONV_FILTERS, kernel_size=CONV_KERNEL_SIZE),\n",
    "    LSTM(units=LSTM_UNITS),\n",
    "    Dense(units=DENSE_UNITS),\n",
    "    Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "print(lstm_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "VALIDATION_SPLIT = 0.25\n",
    "\n",
    "lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "lstm_model.fit(x=sequences, \n",
    "               y=dataset.label.apply(lambda label: 0 if label == 'BBC' else 1),\n",
    "               batch_size=BATCH_SIZE,\n",
    "               validation_split=VALIDATION_SPLIT,\n",
    "               epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geração de Texto\n",
    "\n",
    "2. Usando sua base de textos:\n",
    "\n",
    "* Treine uma rede __LSTM__ para gerar texto, que receba uma ou mais palavras de uma frase como entrada. O treinamento deve ser realizado considerando um conjunto supervisionado que gera a próxima palavra de uma sequência de tamanho 4, usando subsequências de sua base.\n",
    "\n",
    "* Após o treinamento, exiba pelo menos 5 exemplos de textos dados de entrada, e do texto gerado em seguida pela rede treinada. Para cada exemplo, gere pelo menos 10 palavras consecutivamente.\n",
    "\n",
    "* Faça o mesmo usando Cadeias de Markov com bi-grams (usando apenas 1 palavra para tentar prever a seguinte). Compare os resultados com os da __LSTM__.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot_sequence(s):\n",
    "    _ = []\n",
    "    for ts in s:\n",
    "        _.append(list(map(lambda t: tokens_one_hot[t], ts)))\n",
    "    return numpy.array(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(dataset.processed_text[:50])\n",
    "sequences = pad_sequences(sequences, padding='post')\n",
    "\n",
    "tokens = list(tokenizer.word_index.values())[:TOKENIZER_NUM_WORDS]\n",
    "\n",
    "cat_tokens = to_categorical(tokens)\n",
    "\n",
    "tokens_one_hot = {\n",
    "    t: cat_tokens[i] for i, t in enumerate(tokens)\n",
    "}\n",
    "tokens_one_hot[0] = numpy.zeros(shape=TOKENIZER_NUM_WORDS + 1)\n",
    "\n",
    "def generate_subsequences(sequence):\n",
    "  seq, tar = [], []\n",
    "  win = [0, 0, 0, sequence[0]]\n",
    "\n",
    "  for i in sequence[1:]:\n",
    "    seq.append(win.copy()); tar.append(i)\n",
    "    for j in range(1, 4):\n",
    "      win[j-1] = win[j]\n",
    "    win[3] = i\n",
    "  \n",
    "  return seq, tar\n",
    "\n",
    "train_sequences, train_targets = [], []\n",
    "\n",
    "for s in sequences:\n",
    "    seq, tar = generate_subsequences(s)\n",
    "    train_sequences.append(seq)\n",
    "    train_targets.append(tar)\n",
    "\n",
    "train_sequences = get_one_hot_sequence(numpy.array([s for seq in train_sequences for s in seq]))\n",
    "train_targets = numpy.array([tokens_one_hot[t] for tar in train_targets for t in tar])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_14 (LSTM)               (None, 100)               840800    \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 2001)              202101    \n",
      "=================================================================\n",
      "Total params: 1,042,901\n",
      "Trainable params: 1,042,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIMENSION = TOKENIZER_NUM_WORDS + 1\n",
    "EMBEDDING_OUTPUT_DIM = 300\n",
    "INPUT_LENGTH = 4\n",
    "LSTM_UNITS = 100\n",
    "\n",
    "lstm_model = Sequential([\n",
    "    LSTM(units=LSTM_UNITS, input_shape=(INPUT_LENGTH, INPUT_DIMENSION)),\n",
    "    Dense(units=INPUT_DIMENSION, activation='softmax')\n",
    "])\n",
    "\n",
    "print(lstm_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "VALIDATION_SPLIT = 0.25\n",
    "\n",
    "lstm_model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "172/172 [==============================] - 29s 169ms/step - loss: 2.5084 - accuracy: 0.0087 - val_loss: 2.7041 - val_accuracy: 0.0047\n",
      "Epoch 2/20\n",
      "172/172 [==============================] - 17s 98ms/step - loss: 2.4147 - accuracy: 0.0052 - val_loss: 2.7061 - val_accuracy: 0.0047\n",
      "Epoch 3/20\n",
      "172/172 [==============================] - 16s 92ms/step - loss: 2.4117 - accuracy: 0.0052 - val_loss: 2.7148 - val_accuracy: 0.0047\n",
      "Epoch 4/20\n",
      "172/172 [==============================] - 17s 97ms/step - loss: 2.4112 - accuracy: 0.0052 - val_loss: 2.7133 - val_accuracy: 0.0047\n",
      "Epoch 5/20\n",
      "172/172 [==============================] - 16s 93ms/step - loss: 2.4105 - accuracy: 0.0052 - val_loss: 2.7171 - val_accuracy: 0.0047\n",
      "Epoch 6/20\n",
      "172/172 [==============================] - 16s 92ms/step - loss: 2.4104 - accuracy: 0.0049 - val_loss: 2.7175 - val_accuracy: 0.0047\n",
      "Epoch 7/20\n",
      "172/172 [==============================] - 16s 91ms/step - loss: 2.4092 - accuracy: 0.0050 - val_loss: 2.7194 - val_accuracy: 0.0047\n",
      "Epoch 8/20\n",
      "172/172 [==============================] - 16s 93ms/step - loss: 2.4116 - accuracy: 0.0050 - val_loss: 2.7223 - val_accuracy: 0.0047\n",
      "Epoch 9/20\n",
      "172/172 [==============================] - 16s 92ms/step - loss: 2.4097 - accuracy: 0.0050 - val_loss: 2.7240 - val_accuracy: 0.0047\n",
      "Epoch 10/20\n",
      "172/172 [==============================] - 16s 92ms/step - loss: 2.4105 - accuracy: 0.0052 - val_loss: 2.7238 - val_accuracy: 0.0047\n",
      "Epoch 11/20\n",
      "172/172 [==============================] - 16s 93ms/step - loss: 2.4103 - accuracy: 0.0048 - val_loss: 2.7241 - val_accuracy: 0.0047\n",
      "Epoch 12/20\n",
      "172/172 [==============================] - 16s 95ms/step - loss: 2.4104 - accuracy: 0.0049 - val_loss: 2.7256 - val_accuracy: 0.0047\n",
      "Epoch 13/20\n",
      "172/172 [==============================] - 16s 94ms/step - loss: 2.4103 - accuracy: 0.0048 - val_loss: 2.7292 - val_accuracy: 0.0047\n",
      "Epoch 14/20\n",
      "172/172 [==============================] - 16s 95ms/step - loss: 2.4105 - accuracy: 0.0050 - val_loss: 2.7267 - val_accuracy: 0.0047\n",
      "Epoch 15/20\n",
      "172/172 [==============================] - 16s 95ms/step - loss: 2.4100 - accuracy: 0.0051 - val_loss: 2.7286 - val_accuracy: 0.0047\n",
      "Epoch 16/20\n",
      "172/172 [==============================] - 16s 93ms/step - loss: 2.4104 - accuracy: 0.0048 - val_loss: 2.7345 - val_accuracy: 0.0047\n",
      "Epoch 17/20\n",
      "172/172 [==============================] - 16s 93ms/step - loss: 2.4094 - accuracy: 0.0048 - val_loss: 2.7330 - val_accuracy: 0.0047\n",
      "Epoch 18/20\n",
      "172/172 [==============================] - 16s 93ms/step - loss: 2.4109 - accuracy: 0.0048 - val_loss: 2.7313 - val_accuracy: 0.0047\n",
      "Epoch 19/20\n",
      "172/172 [==============================] - 16s 95ms/step - loss: 2.4103 - accuracy: 0.0049 - val_loss: 2.7358 - val_accuracy: 0.0047\n",
      "Epoch 20/20\n",
      "172/172 [==============================] - 17s 98ms/step - loss: 2.4108 - accuracy: 0.0045 - val_loss: 2.7338 - val_accuracy: 0.0047\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22b9d408af0>"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model.fit(\n",
    "    x=train_sequences, \n",
    "    y=train_targets,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    validation_split=VALIDATION_SPLIT,\n",
    "    epochs=20, verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_text(initial, max_len=500):\n",
    "    initial = get_one_hot_sequence(pad_sequences(tokenizer.texts_to_sequences([initial])[:4], 4, padding='post'))\n",
    "    text = []\n",
    "    \n",
    "    print(initial)\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        predict = numpy.argmax(lstm_model.predict([initial]))\n",
    "        initial[0][:-1] = initial[0][1:]; initial[0][-1] = tokens_one_hot[predict]\n",
    "        text.append(tokenizer.index_word[predict])\n",
    "    \n",
    "    return ' '.join(text)\n",
    "\n",
    "gen_text(input('Palavras inicias'), max_len=int(input('Tamanho do texto')))\n",
    "# O coronavírus no Brasil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Processamento de linguagem natural - 4ª lista de exercícios.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
