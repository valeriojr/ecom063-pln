{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Universidade Federal de Alagoas\n",
    "\n",
    "IC - Instituto de Computação\n",
    "\n",
    " \n",
    "\n",
    "# Processamento de linguagem natural - 2020.1\n",
    "**Professor**: Thales Vieira\n",
    "\n",
    "**Alunos**: Hugo Tallys Martins Oliveira e Valério Nogueira Rodrigues Júnior\n",
    "\n",
    "\n",
    "## 6ª lista de exercícios\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pré-processamento dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy\n",
    "import pandas\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import umap\n",
    "import pickle\n",
    "\n",
    "from bokeh.io import output_notebook, show\n",
    "from bokeh.palettes import Category20\n",
    "from bokeh.plotting import figure\n",
    "from gensim.models import KeyedVectors\n",
    "from IPython.display import HTML, display\n",
    "from keras.layers import Conv1D, Dense, Embedding, LSTM\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD, NMF, PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import normalize\n",
    "from yellowbrick.cluster import KElbowVisualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1001\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1001\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.1.1.min.js\": \"kLr4fYcqcSpbuI95brIH3vnnYCquzzSxHPU6XGQCIkQRGJwhg0StNbj1eegrHs12\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.1.1.min.js\": \"xIGPmVtaOm+z0BqfSOMn4lOR6ciex448GIKG4eE61LsAvmGj48XcMQZtKcE/UXZe\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.1.1.min.js\": \"Dc9u1wF/0zApGIWoBbH77iWEHtdmkuYWG839Uzmv8y8yBLXebjO9ZnERsde5Ln/P\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.1.1.min.js\": \"cT9JaBz7GiRXdENrJLZNSC6eMNF3nh3fa5fTF51Svp+ukxPdwcU5kGXGPBgDCa2j\"};\n",
       "\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      if (url in hashes) {\n",
       "        element.crossOrigin = \"anonymous\";\n",
       "        element.integrity = \"sha384-\" + hashes[url];\n",
       "      }\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  \n",
       "  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.1.1.min.js\"];\n",
       "  var css_urls = [];\n",
       "  \n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "    \n",
       "    \n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      \n",
       "    for (var i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "    if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.1.1.min.js\": \"kLr4fYcqcSpbuI95brIH3vnnYCquzzSxHPU6XGQCIkQRGJwhg0StNbj1eegrHs12\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.1.1.min.js\": \"xIGPmVtaOm+z0BqfSOMn4lOR6ciex448GIKG4eE61LsAvmGj48XcMQZtKcE/UXZe\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.1.1.min.js\": \"Dc9u1wF/0zApGIWoBbH77iWEHtdmkuYWG839Uzmv8y8yBLXebjO9ZnERsde5Ln/P\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.1.1.min.js\": \"cT9JaBz7GiRXdENrJLZNSC6eMNF3nh3fa5fTF51Svp+ukxPdwcU5kGXGPBgDCa2j\"};\n\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      if (url in hashes) {\n        element.crossOrigin = \"anonymous\";\n        element.integrity = \"sha384-\" + hashes[url];\n      }\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.1.1.min.js\"];\n  var css_urls = [];\n  \n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (var i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_notebook() # Necessário para visualizar os gráficos com bokeh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Hugo -\n",
      "[nltk_data]     LED\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package rslp to C:\\Users\\Hugo -\n",
      "[nltk_data]     LED\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Hugo -\n",
      "[nltk_data]     LED\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords'); nltk.download('rslp'); nltk.download('punkt');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bbc_dataset_url = 'data/bbc.csv'\n",
    "cnn_dataset_url = 'data/cnn.csv'\n",
    "\n",
    "bbc = pandas.read_csv(bbc_dataset_url, sep='|')\n",
    "bbc['label'] = 'BBC'\n",
    "cnn = pandas.read_csv(cnn_dataset_url, sep='|')\n",
    "cnn['label'] = 'CNN'\n",
    "\n",
    "dataset = pandas.concat([bbc, cnn], ignore_index=True)\n",
    "dataset = dataset.dropna(axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset.text = dataset.text.apply(lambda text: text.replace('\\n', ' ')) # Remoção das quebras de linha\n",
    "dataset.title = dataset.title.apply(lambda text: text.replace('\\n', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def remove_boilerplate(text):\n",
    "    boilerplate = ['Compartilhe este post com Email Facebook Messenger Messenger Twitter WhatsApp LinkedIn Copiar este link Estes são links externos e abrirão numa nova janela', 'Já assistiu aos nossos novos vídeos no YouTube? Inscreva-se no nosso canal!', 'Final de YouTube post  de BBC News Brasil Final de YouTube post 2 de BBC News Brasil Final de YouTube post 3 de BBC News Brasil']\n",
    "    \n",
    "    for b in boilerplate:\n",
    "        text = text.replace(b, '')\n",
    "    return text\n",
    "\n",
    "dataset.text = dataset.text.apply(remove_boilerplate) # Remoção de fragmentos irrelevantes do texto que se repetem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = re.sub(r'\\w*\\d\\w*', '', text) # Remove todas as palavras que contém números\n",
    "    text = re.sub(r'[^a-zA-ZàáéíóúÁÉÍÓÚâêîôÂÊÎÔãõÃÕçÇ ]', '', text.lower()) # Remove pontuação e converte para minúscula\n",
    "    return re.sub(r'\\s+', ' ', text) # Remove espaços repetidos\n",
    "\n",
    "dataset['processed_text'] = dataset.text.apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "\n",
    "def tokenize_remove_stopwords(text, only_tokenize=False):\n",
    "    tokenized_text = nltk.word_tokenize(text, language='portuguese')\n",
    "    \n",
    "    if only_tokenize:\n",
    "        return ' '.join([token for token in tokenized_text])\n",
    "    return ' '.join([token for token in tokenized_text if token not in stopwords])\n",
    "\n",
    "dataset['processed_text_'] = dataset.processed_text.apply(lambda t: tokenize_remove_stopwords(t, True)) # Tokeniza o texto e NÃO remove stopwords\n",
    "dataset.processed_text = dataset.processed_text.apply(lambda t: tokenize_remove_stopwords(t)) # Tokeniza o texto e remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>processed_text</th>\n",
       "      <th>processed_text_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.bbc.co.uk/portuguese/brasil-53020785</td>\n",
       "      <td>Coronavírus: pandemia pode jogar até 14 milhõe...</td>\n",
       "      <td>A turbulência econômica causada pela pandemi...</td>\n",
       "      <td>BBC</td>\n",
       "      <td>turbulência econômica causada pandemia novo co...</td>\n",
       "      <td>a turbulência econômica causada pela pandemia ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.bbc.co.uk/portuguese/brasil-53027318</td>\n",
       "      <td>Coronavírus: como funcionam as duas vacinas co...</td>\n",
       "      <td>Cerca de 11 mil voluntários brasileiros vão ...</td>\n",
       "      <td>BBC</td>\n",
       "      <td>cerca mil voluntários brasileiros vão receber ...</td>\n",
       "      <td>cerca de mil voluntários brasileiros vão receb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.bbc.co.uk/portuguese/brasil-51713943</td>\n",
       "      <td>Coronavírus: Brasil passa o Reino Unido e se t...</td>\n",
       "      <td>*atualizada às 18h20 de 12 de junho de 2020 ...</td>\n",
       "      <td>BBC</td>\n",
       "      <td>atualizada junho brasil totalizou nesta sextaf...</td>\n",
       "      <td>atualizada às de de junho de o brasil totalizo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.bbc.co.uk/portuguese/internacional...</td>\n",
       "      <td>Coronavírus na Índia: com lockdown 'insustentá...</td>\n",
       "      <td>Quando, em 24 de março, o governo indiano in...</td>\n",
       "      <td>BBC</td>\n",
       "      <td>março governo indiano iniciou estrito isolamen...</td>\n",
       "      <td>quando em de março o governo indiano iniciou u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.bbc.co.uk/portuguese/internacional...</td>\n",
       "      <td>2ª onda do coronavírus? Irã vê aumento acelera...</td>\n",
       "      <td>O Irã registrou um rápido aumento no número ...</td>\n",
       "      <td>BBC</td>\n",
       "      <td>irã registrou rápido aumento número casos covi...</td>\n",
       "      <td>o irã registrou um rápido aumento no número de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1611</th>\n",
       "      <td>https://www.cnnbrasil.com.br/saude/2020/02/27/...</td>\n",
       "      <td>Farmácias têm falta de máscaras após confirmaç...</td>\n",
       "      <td>Com a confirmação do primeiro caso de contamin...</td>\n",
       "      <td>CNN</td>\n",
       "      <td>confirmação primeiro caso contaminação novo co...</td>\n",
       "      <td>com a confirmação do primeiro caso de contamin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1612</th>\n",
       "      <td>https://www.cnnbrasil.com.br/business/2020/02/...</td>\n",
       "      <td>Ibovespa tem nova queda com mercado ainda preo...</td>\n",
       "      <td>Preocupações com a propagação do novo coronaví...</td>\n",
       "      <td>CNN</td>\n",
       "      <td>preocupações propagação novo coronavírus poten...</td>\n",
       "      <td>preocupações com a propagação do novo coronaví...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1613</th>\n",
       "      <td>https://www.cnnbrasil.com.br/internacional/202...</td>\n",
       "      <td>Japonesa testa positivo pela segunda vez para ...</td>\n",
       "      <td>TÓQUIO - Uma guia de ônibus turístico no Japão...</td>\n",
       "      <td>CNN</td>\n",
       "      <td>tóquio guia ônibus turístico japão apresentou ...</td>\n",
       "      <td>tóquio uma guia de ônibus turístico no japão a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1614</th>\n",
       "      <td>https://www.cnnbrasil.com.br/saude/2020/02/26/...</td>\n",
       "      <td>Primeiro brasileiro com coronavírus tem sintom...</td>\n",
       "      <td>O primeiro brasileiro com diagnóstico confir...</td>\n",
       "      <td>CNN</td>\n",
       "      <td>primeiro brasileiro diagnóstico confirmado cor...</td>\n",
       "      <td>o primeiro brasileiro com diagnóstico confirma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1615</th>\n",
       "      <td>https://www.cnnbrasil.com.br/saude/2020/02/26/...</td>\n",
       "      <td>Coronavírus: 'Não é momento para pânico' no Br...</td>\n",
       "      <td>O Brasil está preparado para lidar com o cor...</td>\n",
       "      <td>CNN</td>\n",
       "      <td>brasil preparado lidar coronavírus afirmou méd...</td>\n",
       "      <td>o brasil está preparado para lidar com o coron...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1616 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    url  \\\n",
       "0      https://www.bbc.co.uk/portuguese/brasil-53020785   \n",
       "1      https://www.bbc.co.uk/portuguese/brasil-53027318   \n",
       "2      https://www.bbc.co.uk/portuguese/brasil-51713943   \n",
       "3     https://www.bbc.co.uk/portuguese/internacional...   \n",
       "4     https://www.bbc.co.uk/portuguese/internacional...   \n",
       "...                                                 ...   \n",
       "1611  https://www.cnnbrasil.com.br/saude/2020/02/27/...   \n",
       "1612  https://www.cnnbrasil.com.br/business/2020/02/...   \n",
       "1613  https://www.cnnbrasil.com.br/internacional/202...   \n",
       "1614  https://www.cnnbrasil.com.br/saude/2020/02/26/...   \n",
       "1615  https://www.cnnbrasil.com.br/saude/2020/02/26/...   \n",
       "\n",
       "                                                  title  \\\n",
       "0     Coronavírus: pandemia pode jogar até 14 milhõe...   \n",
       "1     Coronavírus: como funcionam as duas vacinas co...   \n",
       "2     Coronavírus: Brasil passa o Reino Unido e se t...   \n",
       "3     Coronavírus na Índia: com lockdown 'insustentá...   \n",
       "4     2ª onda do coronavírus? Irã vê aumento acelera...   \n",
       "...                                                 ...   \n",
       "1611  Farmácias têm falta de máscaras após confirmaç...   \n",
       "1612  Ibovespa tem nova queda com mercado ainda preo...   \n",
       "1613  Japonesa testa positivo pela segunda vez para ...   \n",
       "1614  Primeiro brasileiro com coronavírus tem sintom...   \n",
       "1615  Coronavírus: 'Não é momento para pânico' no Br...   \n",
       "\n",
       "                                                   text label  \\\n",
       "0       A turbulência econômica causada pela pandemi...   BBC   \n",
       "1       Cerca de 11 mil voluntários brasileiros vão ...   BBC   \n",
       "2       *atualizada às 18h20 de 12 de junho de 2020 ...   BBC   \n",
       "3       Quando, em 24 de março, o governo indiano in...   BBC   \n",
       "4       O Irã registrou um rápido aumento no número ...   BBC   \n",
       "...                                                 ...   ...   \n",
       "1611  Com a confirmação do primeiro caso de contamin...   CNN   \n",
       "1612  Preocupações com a propagação do novo coronaví...   CNN   \n",
       "1613  TÓQUIO - Uma guia de ônibus turístico no Japão...   CNN   \n",
       "1614    O primeiro brasileiro com diagnóstico confir...   CNN   \n",
       "1615    O Brasil está preparado para lidar com o cor...   CNN   \n",
       "\n",
       "                                         processed_text  \\\n",
       "0     turbulência econômica causada pandemia novo co...   \n",
       "1     cerca mil voluntários brasileiros vão receber ...   \n",
       "2     atualizada junho brasil totalizou nesta sextaf...   \n",
       "3     março governo indiano iniciou estrito isolamen...   \n",
       "4     irã registrou rápido aumento número casos covi...   \n",
       "...                                                 ...   \n",
       "1611  confirmação primeiro caso contaminação novo co...   \n",
       "1612  preocupações propagação novo coronavírus poten...   \n",
       "1613  tóquio guia ônibus turístico japão apresentou ...   \n",
       "1614  primeiro brasileiro diagnóstico confirmado cor...   \n",
       "1615  brasil preparado lidar coronavírus afirmou méd...   \n",
       "\n",
       "                                        processed_text_  \n",
       "0     a turbulência econômica causada pela pandemia ...  \n",
       "1     cerca de mil voluntários brasileiros vão receb...  \n",
       "2     atualizada às de de junho de o brasil totalizo...  \n",
       "3     quando em de março o governo indiano iniciou u...  \n",
       "4     o irã registrou um rápido aumento no número de...  \n",
       "...                                                 ...  \n",
       "1611  com a confirmação do primeiro caso de contamin...  \n",
       "1612  preocupações com a propagação do novo coronaví...  \n",
       "1613  tóquio uma guia de ônibus turístico no japão a...  \n",
       "1614  o primeiro brasileiro com diagnóstico confirma...  \n",
       "1615  o brasil está preparado para lidar com o coron...  \n",
       "\n",
       "[1616 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER_NUM_WORDS = 2000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=TOKENIZER_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(dataset.processed_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding, convoluções e LSTM\n",
    "\n",
    "Resolva novamente a segunda questão da 3ª lista usando pelo menos duas\n",
    "arquiteturas de redes neurais que utilizem camadas *Embedding*, convolucionais e\n",
    "*LSTM*. Compare com os resultados obtidos anteriormente nas lista 3 e 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gerando as sequências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(dataset.processed_text)\n",
    "sequences = pad_sequences(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIMENSION = TOKENIZER_NUM_WORDS + 1\n",
    "INPUT_LENGTH = 500\n",
    "CONV_FILTERS = 32\n",
    "CONV_KERNEL_SIZE = 3\n",
    "LSTM_UNITS = 100\n",
    "DENSE_UNITS = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = Sequential([\n",
    "    Embedding(input_dim=INPUT_DIMENSION, output_dim=300),\n",
    "    Conv1D(filters=CONV_FILTERS, kernel_size=CONV_KERNEL_SIZE),\n",
    "    LSTM(units=LSTM_UNITS),\n",
    "    Dense(units=DENSE_UNITS),\n",
    "    Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "print(lstm_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "VALIDATION_SPLIT = 0.25\n",
    "\n",
    "lstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "lstm_model.fit(x=sequences, \n",
    "               y=dataset.label.apply(lambda label: 0 if label == 'BBC' else 1),\n",
    "               batch_size=BATCH_SIZE,\n",
    "               validation_split=VALIDATION_SPLIT,\n",
    "               epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geração de Texto\n",
    "\n",
    "2. Usando sua base de textos:\n",
    "\n",
    "* Treine uma rede __LSTM__ para gerar texto, que receba uma ou mais palavras de uma frase como entrada. O treinamento deve ser realizado considerando um conjunto supervisionado que gera a próxima palavra de uma sequência de tamanho 4, usando subsequências de sua base.\n",
    "\n",
    "* Após o treinamento, exiba pelo menos 5 exemplos de textos dados de entrada, e do texto gerado em seguida pela rede treinada. Para cada exemplo, gere pelo menos 10 palavras consecutivamente.\n",
    "\n",
    "* Faça o mesmo usando Cadeias de Markov com bi-grams (usando apenas 1 palavra para tentar prever a seguinte). Compare os resultados com os da __LSTM__.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Começamos tokenizando o texto, mapeando cada palavra $w_i$ do nosso vocabulário a um inteiro $i$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = dataset.processed_text_[:50] # somente os 50 primeiros artigos\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "vocabulary_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Método I - Rede Neural LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "sequences = pad_sequences(sequences, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_subsequences(sequence):\n",
    "    seq, tar = [], []\n",
    "    win = [0, 0, 0, sequence[0]]\n",
    "\n",
    "    for i in sequence[1:]:\n",
    "        seq.append(win.copy()); tar.append(i)\n",
    "        for j in range(1, 4):\n",
    "            win[j-1] = win[j]\n",
    "        win[3] = i\n",
    "\n",
    "    return seq, tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences, train_targets = [], []\n",
    "\n",
    "for s in sequences:\n",
    "    seq, tar = generate_subsequences(s)\n",
    "    train_sequences.append(seq)\n",
    "    train_targets.append(tar)\n",
    "\n",
    "train_sequences = numpy.array([s for train_seq in train_sequences for s in train_seq])\n",
    "train_targets = to_categorical([t for train_tar in train_targets for t in train_tar]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(151050, 4)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(151050, 7797)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 4, 200)            1559400   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 4, 100)            120400    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7797)              787497    \n",
      "=================================================================\n",
      "Total params: 2,557,797\n",
      "Trainable params: 2,557,797\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "INPUT_LENGTH = 4\n",
    "LSTM_UNITS = 100\n",
    "\n",
    "lstm_model = Sequential([\n",
    "    Embedding(input_dim=vocabulary_size, output_dim=200, input_length=INPUT_LENGTH),\n",
    "    LSTM(units=LSTM_UNITS, return_sequences=True),\n",
    "    LSTM(units=LSTM_UNITS),\n",
    "    Dense(units=LSTM_UNITS, activation='relu'),\n",
    "    Dense(units=vocabulary_size, activation='softmax')\n",
    "])\n",
    "\n",
    "print(lstm_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2361/2361 [==============================] - 106s 45ms/step - loss: 2.5739 - accuracy: 0.6577\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d9eb5e0e50>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "lstm_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "lstm_model.fit(\n",
    "    x=train_sequences, \n",
    "    y=train_targets,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=1,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(seed, n):\n",
    "    words = seed.split(' ')\n",
    "    words = [tokenizer.word_index[w] for w in words]\n",
    "    predicted_words = []\n",
    "    \n",
    "    for _ in range(n):\n",
    "        predict = numpy.argmax(lstm_model.predict([words])[0])\n",
    "        predict_word = tokenizer.index_word[predict]\n",
    "        predicted_words.append(predict_word)\n",
    "        for i in range(1, 3):\n",
    "            words[i] = words[i+1]\n",
    "        words[3] = predict\n",
    "    \n",
    "    return seed + ' ' + ' '.join(predicted_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemplo de sentença gerada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a pandemia do coronavírus de que de que de que de que de que de que de que de'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sentence(seed='a pandemia do coronavírus', n=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvando o modelo e o tokenizador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm_model.save('./model.h5')\n",
    "# pickle.dump(tokenizer, open('./tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregando o modelo já treinado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = load_model('./text_generation/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'presidente jair bolsonaro disse que faleceu com a mulher afinidade a decisão do instituto'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sentence('presidente jair bolsonaro disse', n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a pandemia do coronavírus começou a aliviar a partir do número oficial o que'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sentence('a pandemia do coronavírus', n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'o governo do brasil realizado à monarca de anos centenas de cuidados à comunidade'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sentence('o governo do brasil', n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a criação da vacina em que o novo coronavírus cai apenas entre entre e'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sentence('a criação da vacina', n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Método II - Cadeias de Markov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_matrix = numpy.zeros(shape=(vocabulary_size, vocabulary_size))\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "for seq in sequences:\n",
    "    for i in range(len(seq)):\n",
    "        if i < len(seq) - 1:\n",
    "            transition_matrix[seq[i]][seq[i+1]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sentece(initial_word, n=20):\n",
    "    initial_token = tokenizer.word_index[initial_word]\n",
    "    result = [initial_word]\n",
    "    \n",
    "    for _ in range(n):\n",
    "        probs = transition_matrix[initial_token] / transition_matrix[initial_token].sum()\n",
    "        next_token = numpy.random.choice(vocabulary_size, 1, p=probs)[0]\n",
    "        result.append(tokenizer.index_word[next_token])\n",
    "        initial_token = next_token\n",
    "        \n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palavra de entrada: presidente\n",
      "Sentença gerada: \" presidente jair bolsonaro brincou com seu território brasileiro nega a pandemia \"\n",
      "\n",
      "Palavra de entrada: vírus\n",
      "Sentença gerada: \" vírus respiratórios fatais o modo muito mais pobres do país entre \"\n",
      "\n",
      "Palavra de entrada: pandemia\n",
      "Sentença gerada: \" pandemia de autossuficiência alimentar se for interrompido a pele das às \"\n",
      "\n",
      "Palavra de entrada: brasil\n",
      "Sentença gerada: \" brasil o piauiense passou a ver com o conselho nacional fica \"\n",
      "\n",
      "Palavra de entrada: vacina\n",
      "Sentença gerada: \" vacina nacional de casos como também porque não considera nessa época \"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for word in ['presidente', 'vírus', 'pandemia', 'brasil', 'vacina']:\n",
    "    print('Palavra de entrada: %s' % word)\n",
    "    print('Sentença gerada: \" %s \"\\n' % build_sentece(initial_word=word, n=10))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Processamento de linguagem natural - 4ª lista de exercícios.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
