{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Universidade Federal de Alagoas\n",
    "\n",
    "IC - Instituto de Computação\n",
    "\n",
    " \n",
    "\n",
    "# Processamento de linguagem natural - 2020.1\n",
    "**Professor**: Thales Vieira\n",
    "\n",
    "**Alunos**: Hugo Tallys Martins Oliveira e Valério Nogueira Rodrigues Júnior\n",
    "\n",
    "\n",
    "## 5ª lista de exercícios\n",
    "\n",
    "---"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pré-processamento dos dados"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import umap\n",
    "import nltk\n",
    "import numpy\n",
    "import pandas\n",
    "import random\n",
    "\n",
    "from bokeh.plotting import figure\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from bokeh.palettes import Category20\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.decomposition import PCA\n",
    "from IPython.display import HTML, display\n",
    "from bokeh.io import output_notebook, show\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD, NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/html": "\n    <div class=\"bk-root\">\n        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n        <span id=\"1002\">Loading BokehJS ...</span>\n    </div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  var JS_MIME_TYPE = 'application/javascript';\n  var HTML_MIME_TYPE = 'text/html';\n  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n  var CLASS_NAME = 'output_bokeh rendered_html';\n\n  /**\n   * Render data to the DOM node\n   */\n  function render(props, node) {\n    var script = document.createElement(\"script\");\n    node.appendChild(script);\n  }\n\n  /**\n   * Handle when an output is cleared or removed\n   */\n  function handleClearOutput(event, handle) {\n    var cell = handle.cell;\n\n    var id = cell.output_area._bokeh_element_id;\n    var server_id = cell.output_area._bokeh_server_id;\n    // Clean up Bokeh references\n    if (id != null && id in Bokeh.index) {\n      Bokeh.index[id].model.document.clear();\n      delete Bokeh.index[id];\n    }\n\n    if (server_id !== undefined) {\n      // Clean up Bokeh references\n      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n      cell.notebook.kernel.execute(cmd, {\n        iopub: {\n          output: function(msg) {\n            var id = msg.content.text.trim();\n            if (id in Bokeh.index) {\n              Bokeh.index[id].model.document.clear();\n              delete Bokeh.index[id];\n            }\n          }\n        }\n      });\n      // Destroy server and session\n      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n      cell.notebook.kernel.execute(cmd);\n    }\n  }\n\n  /**\n   * Handle when a new output is added\n   */\n  function handleAddOutput(event, handle) {\n    var output_area = handle.output_area;\n    var output = handle.output;\n\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n      return\n    }\n\n    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n      // store reference to embed id on output_area\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n    }\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n      var bk_div = document.createElement(\"div\");\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n      var script_attrs = bk_div.children[0].attributes;\n      for (var i = 0; i < script_attrs.length; i++) {\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n      }\n      // store reference to server id on output_area\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n    }\n  }\n\n  function register_renderer(events, OutputArea) {\n\n    function append_mime(data, metadata, element) {\n      // create a DOM node to render to\n      var toinsert = this.create_output_subarea(\n        metadata,\n        CLASS_NAME,\n        EXEC_MIME_TYPE\n      );\n      this.keyboard_manager.register_events(toinsert);\n      // Render to node\n      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n      render(props, toinsert[toinsert.length - 1]);\n      element.append(toinsert);\n      return toinsert\n    }\n\n    /* Handle when an output is cleared or removed */\n    events.on('clear_output.CodeCell', handleClearOutput);\n    events.on('delete.Cell', handleClearOutput);\n\n    /* Handle when a new output is added */\n    events.on('output_added.OutputArea', handleAddOutput);\n\n    /**\n     * Register the mime type and append_mime function with output_area\n     */\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n      /* Is output safe? */\n      safe: true,\n      /* Index of renderer in `output_area.display_order` */\n      index: 0\n    });\n  }\n\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\n  if (root.Jupyter !== undefined) {\n    var events = require('base/js/events');\n    var OutputArea = require('notebook/js/outputarea').OutputArea;\n\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  }\n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1002\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.1.1.min.js\": \"kLr4fYcqcSpbuI95brIH3vnnYCquzzSxHPU6XGQCIkQRGJwhg0StNbj1eegrHs12\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.1.1.min.js\": \"xIGPmVtaOm+z0BqfSOMn4lOR6ciex448GIKG4eE61LsAvmGj48XcMQZtKcE/UXZe\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.1.1.min.js\": \"Dc9u1wF/0zApGIWoBbH77iWEHtdmkuYWG839Uzmv8y8yBLXebjO9ZnERsde5Ln/P\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.1.1.min.js\": \"cT9JaBz7GiRXdENrJLZNSC6eMNF3nh3fa5fTF51Svp+ukxPdwcU5kGXGPBgDCa2j\"};\n\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      if (url in hashes) {\n        element.crossOrigin = \"anonymous\";\n        element.integrity = \"sha384-\" + hashes[url];\n      }\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.1.1.min.js\"];\n  var css_urls = [];\n  \n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (var i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1002\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));",
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1002\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    const hashes = {\"https://cdn.bokeh.org/bokeh/release/bokeh-2.1.1.min.js\": \"kLr4fYcqcSpbuI95brIH3vnnYCquzzSxHPU6XGQCIkQRGJwhg0StNbj1eegrHs12\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.1.1.min.js\": \"xIGPmVtaOm+z0BqfSOMn4lOR6ciex448GIKG4eE61LsAvmGj48XcMQZtKcE/UXZe\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.1.1.min.js\": \"Dc9u1wF/0zApGIWoBbH77iWEHtdmkuYWG839Uzmv8y8yBLXebjO9ZnERsde5Ln/P\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.1.1.min.js\": \"cT9JaBz7GiRXdENrJLZNSC6eMNF3nh3fa5fTF51Svp+ukxPdwcU5kGXGPBgDCa2j\"};\n\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      if (url in hashes) {\n        element.crossOrigin = \"anonymous\";\n        element.integrity = \"sha384-\" + hashes[url];\n      }\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.1.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.1.1.min.js\"];\n  var css_urls = [];\n  \n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (var i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1002\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_notebook() # Necessário para visualizar os gráficos com bokeh"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Valerio\n",
      "[nltk_data]     Nogueira\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package rslp to C:\\Users\\Valerio\n",
      "[nltk_data]     Nogueira\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Valerio\n",
      "[nltk_data]     Nogueira\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords'); nltk.download('rslp'); nltk.download('punkt');"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "bbc_dataset_url = 'data/bbc.csv'\n",
    "cnn_dataset_url = 'data/cnn.csv'\n",
    "\n",
    "bbc = pandas.read_csv(bbc_dataset_url, sep='|')\n",
    "bbc['label'] = 'BBC'\n",
    "cnn = pandas.read_csv(cnn_dataset_url, sep='|')\n",
    "cnn['label'] = 'CNN'\n",
    "\n",
    "dataset = pandas.concat([bbc, cnn], ignore_index=True)\n",
    "dataset = dataset.dropna(axis=0).reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "dataset.text = dataset.text.apply(lambda text: text.replace('\\n', ' ')) # Remoção das quebras de linha\n",
    "dataset.title = dataset.title.apply(lambda text: text.replace('\\n', ' '))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def remove_boilerplate(text):\n",
    "    boilerplate = ['Compartilhe este post com Email Facebook Messenger Messenger Twitter WhatsApp LinkedIn Copiar este link Estes são links externos e abrirão numa nova janela', 'Já assistiu aos nossos novos vídeos no YouTube? Inscreva-se no nosso canal!', 'Final de YouTube post  de BBC News Brasil Final de YouTube post 2 de BBC News Brasil Final de YouTube post 3 de BBC News Brasil']\n",
    "    \n",
    "    for b in boilerplate:\n",
    "        text = text.replace(b, '')\n",
    "    return text\n",
    "\n",
    "dataset.text = dataset.text.apply(remove_boilerplate) # Remoção de fragmentos irrelevantes do texto que se repetem"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = re.sub(r'\\w*\\d\\w*', '', text) # Remove todas as palavras que contém números\n",
    "    text = re.sub(r'[^a-zA-ZáéíóúÁÉÍÓÚâêîôÂÊÎÔãõÃÕçÇ ]', '', text.lower()) # Remove pontuação e converte para minúscula\n",
    "    return re.sub(r'\\s+', ' ', text) # Remove espaços repetidos\n",
    "\n",
    "dataset['processed_text'] = dataset.text.apply(preprocess)\n",
    "dataset['processed_title'] = dataset.title.apply(preprocess)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "\n",
    "def tokenize_remove_stopwords(text):\n",
    "    tokenized_text = nltk.word_tokenize(text, language='portuguese')\n",
    "    return \" \".join([token for token in tokenized_text if token not in stopwords])\n",
    "\n",
    "dataset.processed_text = dataset.processed_text.apply(tokenize_remove_stopwords) # Tokeniza o texto e remove stopwords\n",
    "dataset.processed_title = dataset.processed_title.apply(tokenize_remove_stopwords) # Tokeniza o titulo e remove stopwords"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                    url  \\\n0      https://www.bbc.co.uk/portuguese/brasil-53020785   \n1      https://www.bbc.co.uk/portuguese/brasil-53027318   \n2      https://www.bbc.co.uk/portuguese/brasil-51713943   \n3     https://www.bbc.co.uk/portuguese/internacional...   \n4     https://www.bbc.co.uk/portuguese/internacional...   \n...                                                 ...   \n1611  https://www.cnnbrasil.com.br/saude/2020/02/27/...   \n1612  https://www.cnnbrasil.com.br/business/2020/02/...   \n1613  https://www.cnnbrasil.com.br/internacional/202...   \n1614  https://www.cnnbrasil.com.br/saude/2020/02/26/...   \n1615  https://www.cnnbrasil.com.br/saude/2020/02/26/...   \n\n                                                  title  \\\n0     Coronavírus: pandemia pode jogar até 14 milhõe...   \n1     Coronavírus: como funcionam as duas vacinas co...   \n2     Coronavírus: Brasil passa o Reino Unido e se t...   \n3     Coronavírus na Índia: com lockdown 'insustentá...   \n4     2ª onda do coronavírus? Irã vê aumento acelera...   \n...                                                 ...   \n1611  Farmácias têm falta de máscaras após confirmaç...   \n1612  Ibovespa tem nova queda com mercado ainda preo...   \n1613  Japonesa testa positivo pela segunda vez para ...   \n1614  Primeiro brasileiro com coronavírus tem sintom...   \n1615  Coronavírus: 'Não é momento para pânico' no Br...   \n\n                                                   text label  \\\n0       A turbulência econômica causada pela pandemi...   BBC   \n1       Cerca de 11 mil voluntários brasileiros vão ...   BBC   \n2       *atualizada às 18h20 de 12 de junho de 2020 ...   BBC   \n3       Quando, em 24 de março, o governo indiano in...   BBC   \n4       O Irã registrou um rápido aumento no número ...   BBC   \n...                                                 ...   ...   \n1611  Com a confirmação do primeiro caso de contamin...   CNN   \n1612  Preocupações com a propagação do novo coronaví...   CNN   \n1613  TÓQUIO - Uma guia de ônibus turístico no Japão...   CNN   \n1614    O primeiro brasileiro com diagnóstico confir...   CNN   \n1615    O Brasil está preparado para lidar com o cor...   CNN   \n\n                                         processed_text  \\\n0     turbulência econômica causada pandemia novo co...   \n1     cerca mil voluntários brasileiros vão receber ...   \n2     atualizada s junho brasil totalizou nesta sext...   \n3     março governo indiano iniciou estrito isolamen...   \n4     irã registrou rápido aumento número casos covi...   \n...                                                 ...   \n1611  confirmação primeiro caso contaminação novo co...   \n1612  preocupações propagação novo coronavírus poten...   \n1613  tóquio guia ônibus turístico japão apresentou ...   \n1614  primeiro brasileiro diagnóstico confirmado cor...   \n1615  brasil preparado lidar coronavírus afirmou méd...   \n\n                                        processed_title  \n0     coronavírus pandemia pode jogar milhões brasil...  \n1     coronavírus funcionam duas vacinas contra covi...  \n2     coronavírus brasil passa reino unido torna seg...  \n3     coronavírus índia lockdown insustentável índia...  \n4     onda coronavírus irã vê aumento acelerado após...  \n...                                                 ...  \n1611  farmácias têm falta máscaras após confirmação ...  \n1612  ibovespa nova queda mercado ainda preocupado c...  \n1613    japonesa testa positivo segunda vez coronavírus  \n1614  primeiro brasileiro coronavírus sintomas brand...  \n1615  coronavírus momento pânico brasil diz médico s...  \n\n[1616 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>url</th>\n      <th>title</th>\n      <th>text</th>\n      <th>label</th>\n      <th>processed_text</th>\n      <th>processed_title</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>https://www.bbc.co.uk/portuguese/brasil-53020785</td>\n      <td>Coronavírus: pandemia pode jogar até 14 milhõe...</td>\n      <td>A turbulência econômica causada pela pandemi...</td>\n      <td>BBC</td>\n      <td>turbulência econômica causada pandemia novo co...</td>\n      <td>coronavírus pandemia pode jogar milhões brasil...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>https://www.bbc.co.uk/portuguese/brasil-53027318</td>\n      <td>Coronavírus: como funcionam as duas vacinas co...</td>\n      <td>Cerca de 11 mil voluntários brasileiros vão ...</td>\n      <td>BBC</td>\n      <td>cerca mil voluntários brasileiros vão receber ...</td>\n      <td>coronavírus funcionam duas vacinas contra covi...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>https://www.bbc.co.uk/portuguese/brasil-51713943</td>\n      <td>Coronavírus: Brasil passa o Reino Unido e se t...</td>\n      <td>*atualizada às 18h20 de 12 de junho de 2020 ...</td>\n      <td>BBC</td>\n      <td>atualizada s junho brasil totalizou nesta sext...</td>\n      <td>coronavírus brasil passa reino unido torna seg...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>https://www.bbc.co.uk/portuguese/internacional...</td>\n      <td>Coronavírus na Índia: com lockdown 'insustentá...</td>\n      <td>Quando, em 24 de março, o governo indiano in...</td>\n      <td>BBC</td>\n      <td>março governo indiano iniciou estrito isolamen...</td>\n      <td>coronavírus índia lockdown insustentável índia...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>https://www.bbc.co.uk/portuguese/internacional...</td>\n      <td>2ª onda do coronavírus? Irã vê aumento acelera...</td>\n      <td>O Irã registrou um rápido aumento no número ...</td>\n      <td>BBC</td>\n      <td>irã registrou rápido aumento número casos covi...</td>\n      <td>onda coronavírus irã vê aumento acelerado após...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1611</th>\n      <td>https://www.cnnbrasil.com.br/saude/2020/02/27/...</td>\n      <td>Farmácias têm falta de máscaras após confirmaç...</td>\n      <td>Com a confirmação do primeiro caso de contamin...</td>\n      <td>CNN</td>\n      <td>confirmação primeiro caso contaminação novo co...</td>\n      <td>farmácias têm falta máscaras após confirmação ...</td>\n    </tr>\n    <tr>\n      <th>1612</th>\n      <td>https://www.cnnbrasil.com.br/business/2020/02/...</td>\n      <td>Ibovespa tem nova queda com mercado ainda preo...</td>\n      <td>Preocupações com a propagação do novo coronaví...</td>\n      <td>CNN</td>\n      <td>preocupações propagação novo coronavírus poten...</td>\n      <td>ibovespa nova queda mercado ainda preocupado c...</td>\n    </tr>\n    <tr>\n      <th>1613</th>\n      <td>https://www.cnnbrasil.com.br/internacional/202...</td>\n      <td>Japonesa testa positivo pela segunda vez para ...</td>\n      <td>TÓQUIO - Uma guia de ônibus turístico no Japão...</td>\n      <td>CNN</td>\n      <td>tóquio guia ônibus turístico japão apresentou ...</td>\n      <td>japonesa testa positivo segunda vez coronavírus</td>\n    </tr>\n    <tr>\n      <th>1614</th>\n      <td>https://www.cnnbrasil.com.br/saude/2020/02/26/...</td>\n      <td>Primeiro brasileiro com coronavírus tem sintom...</td>\n      <td>O primeiro brasileiro com diagnóstico confir...</td>\n      <td>CNN</td>\n      <td>primeiro brasileiro diagnóstico confirmado cor...</td>\n      <td>primeiro brasileiro coronavírus sintomas brand...</td>\n    </tr>\n    <tr>\n      <th>1615</th>\n      <td>https://www.cnnbrasil.com.br/saude/2020/02/26/...</td>\n      <td>Coronavírus: 'Não é momento para pânico' no Br...</td>\n      <td>O Brasil está preparado para lidar com o cor...</td>\n      <td>CNN</td>\n      <td>brasil preparado lidar coronavírus afirmou méd...</td>\n      <td>coronavírus momento pânico brasil diz médico s...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1616 rows × 6 columns</p>\n</div>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Buscador de documentos - Word2vec\n",
    "\n",
    "Desenvolver um buscador de documentos:\n",
    "\n",
    "1. Escolha e aplique um modelo do tipo _word2vec_ a seus textos, compatível com o idioma escolhido (inglês ou português).\n",
    "2. Escolha 5 palavras de consulta que não estão em nenhum dos textos. Para cada palavra de consulta, encontre as 3 palavras __de seu conjunto de textos__ mais parecidas com cada uma das palavras de consulta e exiba os documentos onde estas palavras aparecem.\n",
    "3. Usando as mesmas palavras do item acima, recupere os 3 documentos cujo _word vector_ médio é mais próximo de cada palavra de consulta.\n",
    "4. Realize o procedimento acima usando três modelos com dimensão distinta."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Primeiro vamos determinar o vocabulário do nosso dataset (__corpo do documento__ e __título do documento__):"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "def get_vocab(texts):\n",
    "    vocab = ' '.join([text for text in texts])\n",
    "    return sorted(set(vocab.split(' ')))\n",
    "\n",
    "processed_texts_vocab = get_vocab(dataset.processed_text.values)\n",
    "processed_titles_vocab = get_vocab(dataset.processed_title.values)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sorteando 5 palavras aleatoriamente do vocabulário de títulos:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "['bi', 'curado', 'empresas', 'levar', 'jogos']"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_words = random.sample(processed_titles_vocab, 5)\n",
    "search_words"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Para determinar as palavras mais parecidas a cada palavra de busca selecionada vamos utilizar os seguintes modelos pré-treinados em português:\n",
    "\n",
    "* CBOW 100\n",
    "* SKIPGRAM 50\n",
    "* SKIPGRAM 300\n",
    "\n",
    "A descrição completa do modelo pré-treinado _word2vec_ pode ser encontrada no link:\n",
    "\n",
    "> http://nilc.icmc.usp.br/nilc/index.php/repositorio-de-word-embeddings-do-nilc"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "CBOW_100 = 'C:/Users/Valerio Nogueira/Documents/embeddings/cbow_s100.txt'\n",
    "SKIP_GRAM_100 = 'C:/Users/Valerio Nogueira/Documents/embeddings/skip_s50.txt'\n",
    "SKIP_GRAM_300 = 'C:/Users/Valerio Nogueira/Documents/embeddings/skip_s300.txt'\n",
    "\n",
    "MODEL_LIST = [('CBOW_100', CBOW_100), ('SKIP_GRAM_100', SKIP_GRAM_100), ('SKIP_GRAM_300', SKIP_GRAM_300)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Encontrando lista de palavras mais similares para cada modelo:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Carregando o modelo CBOW_100\n",
      "Calculando palavras mais similares\n",
      "\tbi -> ['th', 'tri', 'g']\n",
      "\tcurado -> ['curada', 'recuperado', 'enganado']\n",
      "\tempresas -> ['montadoras', 'companhias', 'distribuidoras']\n",
      "\tlevar -> ['trazer', 'entregar', 'devolver']\n",
      "\tjogos -> ['torneios', 'campeonatos', 'estádios']\n",
      "\n",
      "Carregando o modelo SKIP_GRAM_100\n",
      "Calculando palavras mais similares\n",
      "\tbi -> ['esp', 'r', 'aa']\n",
      "\tcurado -> ['curada', 'doente', 'pegado']\n",
      "\tempresas -> ['companhias', 'cooperativas', 'seguradoras']\n",
      "\tlevar -> ['trazer', 'estender', 'acompanhar']\n",
      "\tjogos -> ['torneios', 'paralímpicos', 'olímpicos']\n",
      "\n",
      "Carregando o modelo SKIP_GRAM_300\n",
      "Calculando palavras mais similares\n",
      "\tbi -> ['u', 'f', 'trilhão']\n",
      "\tcurado -> ['curada', 'desidratado', 'comido']\n",
      "\tempresas -> ['multinacionais', 'seguradoras', 'companhias']\n",
      "\tlevar -> ['trazer', 'conduzir', 'ir']\n",
      "\tjogos -> ['paralímpicos', 'olímpicos', 'verão']\n"
     ]
    }
   ],
   "source": [
    "def get_most_similar_words(search_word, n):\n",
    "    most_similar = []\n",
    "    for word in processed_texts_vocab:\n",
    "        try:\n",
    "            similarity = embedding.similarity(word, search_word)\n",
    "        except:\n",
    "            similarity = -1\n",
    "        most_similar.append(similarity)\n",
    "    most_similar = numpy.argsort(most_similar)[-(n+1):-1]\n",
    "    most_similar = most_similar[::-1]\n",
    "    return [processed_texts_vocab[i] for i in most_similar]\n",
    "\n",
    "most_similar_words = {}\n",
    "for model_name, model_path in MODEL_LIST:\n",
    "    print('\\nCarregando o modelo %s' % model_name)\n",
    "    embedding = KeyedVectors.load_word2vec_format(model_path)\n",
    "    print('Calculando palavras mais similares')\n",
    "    most_similar_words[model_name] = []\n",
    "    for search_word in search_words:\n",
    "        sim_search_word = get_most_similar_words(search_word, 3)\n",
    "        print(f'\\t{search_word} -> {str(sim_search_word)}')\n",
    "        most_similar_words[model_name].append({'search_word': search_word, 'similar_words': sim_search_word})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dado o __modelo__ e a __palavra de busca__ recuperamos alguns documentos que cotenham as palavras mais semelhantes encontradas no item anterior:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_word_list(model_name, search_word):\n",
    "    for word_list in most_similar_words[model_name]:\n",
    "        if search_word == word_list['search_word']:\n",
    "            return word_list['similar_words']\n",
    "\n",
    "def fetch_documents(word_list):\n",
    "    dataframes = []\n",
    "    for word in word_list:\n",
    "        df = dataset[dataset.processed_text.apply(lambda text: word in text.split(' '))]\n",
    "        dataframes.append(df[['url', 'title']])\n",
    "    return pandas.concat(dataframes).drop_duplicates().reset_index(drop=True)        \n",
    "\n",
    "for model_name, model_path in MODEL_LIST:\n",
    "    for search_word in search_words:\n",
    "        word_list = get_word_list(model_name=model_name, search_word=search_word)\n",
    "        search_documents = fetch_documents(word_list)\n",
    "        display(HTML(f'<h3>Modelo [{model_name}]<br><br>Palavra Busca [{search_word}]<br><br>Palavras Similares {word_list}</h3>'))\n",
    "        display(HTML(search_documents.head().to_html()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A rotina abaixo calcula os três documentos onde seu __vetor médio__ é mais semelhante ao vetor de cada palavra de busca fornecida:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calculate_mean_word2vec(text):\n",
    "    vectors = []\n",
    "    for word in text.split():\n",
    "        try:\n",
    "            vectors.append(word_vectors[word.strip()])\n",
    "        except:\n",
    "            pass\n",
    "    return numpy.mean(vectors, axis=0)\n",
    "    \n",
    "def most_similar_rows(df, col_name, vector, top_n=3):\n",
    "    df['cosine'] = df[col_name].apply(lambda a: numpy.dot(a, vector) / (numpy.linalg.norm(a) * numpy.linalg.norm(vector)))\n",
    "    df = df.sort_values(by=['cosine'], ascending=False)\n",
    "    return df[['url', 'title']][:top_n]\n",
    "\n",
    "\n",
    "# Calcula o vetor médio de cada texto\n",
    "dataset['mean_word_vector'] = dataset.text.apply(calculate_mean_word2vec)\n",
    "\n",
    "for model_name, model_path in MODEL_LIST:\n",
    "    embedding = KeyedVectors.load_word2vec_format(model_path)\n",
    "    word_vectors = embedding.wv\n",
    "    display(HTML('<h2>Modelo - %s</h2>' % model_name))\n",
    "    for search_word in search_words:\n",
    "        display(HTML(f'<h3>{search_word}</h3>'))\n",
    "        display(HTML(most_similar_rows(dataset, 'mean_word_vector', word_vectors[search_word]).to_html()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Representação Doc2Vec"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Vamos treinar um modelo de representação _doc2vec_ para os nossos documentos. Dado o conjunto de textos (lista `paragraphs`) vamos gerar a representação vetorial `doc_embedding`:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "paragraphs = dataset.processed_text.values\n",
    "paragraphs = [paragraph.split(' ') for paragraph in paragraphs]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(paragraphs)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "doc_embedding = Doc2Vec(documents, vector_size=300, window=10, min_count=2, workers=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = numpy.array([doc_embedding[i] for i, text in enumerate(dataset.processed_text.values)])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Classificadores"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Balanceando o dataset para conter 350 artigos _BBC_ e 350 artigos _CNN_ (700 artigos no total):"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = X[:2*bbc.shape[0]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "label2index = {\n",
    "    'BBC': 0, 'CNN': 1\n",
    "}\n",
    "y = [label2index[l] for l in dataset.label.values[:700]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Separando o conjunto de treinamento e teste:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Treinando os classificadores:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Regressão Logística"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Treinamento (TF-IFD):"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "classifier = LogisticRegression().fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Resultado da predição (TF-IDF):"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Matriz de confusão (TF-IDF):"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "from sklearn.metrics import plot_confusion_matrix"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_confusion_matrix(classifier, X_test, y_test)\n",
    "pyplot.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Métricas de validação (TF-IDF):"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "log_metrics = {\n",
    "    'Modelo': 'Regressão Logísitca + TF-IDF',\n",
    "    'Acurácia': accuracy_score(y_test, y_pred),\n",
    "    'Precisão': precision_score(y_test, y_pred),\n",
    "    'Recall': recall_score(y_test, y_pred),\n",
    "    'F1 Score': f1_score(y_test, y_pred)\n",
    "}\n",
    "log_metrics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Naive Bayes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Support Vector Machines (SVM)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Treinamento (TF-IFD):"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "classifier = SVC().fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Resultado da predição (TF-IDF):"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Matriz de confusão (TF-IDF):"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_confusion_matrix(classifier, X_test, y_test)\n",
    "pyplot.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Métricas de validação (TF-IDF):"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "svm_metrics = {\n",
    "    'Modelo': 'SVM + TF-IDF',\n",
    "    'Acurácia': accuracy_score(y_test, y_pred),\n",
    "    'Precisão': precision_score(y_test, y_pred),\n",
    "    'Recall': recall_score(y_test, y_pred),\n",
    "    'F1 Score': f1_score(y_test, y_pred)\n",
    "}\n",
    "svm_metrics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Comparando os classificadores:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pandas.DataFrame([log_metrics, svm_metrics])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Agrupamento\n",
    "\n",
    "Realizar um agrupamento dos dados através das seguintes etapas:\n",
    "\n",
    "* Aplicar o algotimo __PCA__ preservando _95%_ da variância nos dados\n",
    "* Aplicar o algoritmo ___k_-means__ nos dados projetados, utilizando o método _elbow_ para encontrar o valor de _k_ ótimo"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Vamos realizar _stemming_ no texto pré-processado e vetorizar o resultado:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_proj = PCA(n_components=.95).fit_transform(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dimensão dos pontos projetados: "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_proj.shape[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plotando o gráfico da distorção:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "visualizer = KElbowVisualizer(KMeans(), k=(15, 30), metric='distortion')\n",
    "visualizer.fit(X_proj)\n",
    "visualizer.show() "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Observando o gráfico acima escolhemos $k=20$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "k = 20\n",
    "kmeans = KMeans(n_clusters=k).fit(X_proj)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Projetando os pontos no espaço de visualização via __TSNE__:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "X_tsne = TSNE(n_components=2).fit_transform(X_proj)\n",
    "interval_time = time.time() - start_time"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tempo de execução:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('%s segundos' % interval_time)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Gráfico de dispersão dos pontos:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "color_palette = Category20[k]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def scatter_plot(X, labels):\n",
    "    scatter_plot = figure(plot_width=1000, plot_height=500)\n",
    "    scatter_plot.circle(X[:, 0], X[:, 1], size=10, line_color=[color_palette[l] for l in labels], fill_color=[color_palette[l] for l in labels], fill_alpha=.8)\n",
    "    show(scatter_plot)\n",
    "\n",
    "scatter_plot(X_tsne, kmeans.labels_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Projetando os pontos no espaço de visualização via __UMAP__:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "X_umap = umap.UMAP().fit_transform(X_proj)\n",
    "interval_time = time.time() - start_time"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tempo de execução:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('%s segundos' % interval_time)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scatter_plot(X_umap, kmeans.labels_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Processamento de linguagem natural - 4ª lista de exercícios.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}